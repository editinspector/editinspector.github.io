<!DOCTYPE html>
<style>
    .footer {
        font-size: small !important;
        padding: 1rem 1.5rem 1rem !important;
        text-align: center !important;
    }
</style>

<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Academic Project Page</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">EditInspector: A Benchmark for Evaluation of Text-Guided
                        Image Edits</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="https://scholar.google.com/citations?user=mhdOCG4AAAAJ&hl=en"
                   target="_blank">Ron Yosef</a><sup>1</sup>,</span>
                        <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=ZsXf6OMAAAAJ&hl=en"
                     target="_blank">Moran Yanuka</a><sup>2</sup>,</span>
                        <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=P9Fpf4sAAAAJ&hl=en" target="_blank">Yonatan Bitton<sup>3</sup></a>
                  </span>
                        <span class="author-block">
                    <a href="https://scholar.google.ca/citations?user=haahCZ4AAAAJ&hl=en" target="_blank">Dani Lischinski<sup>1,3</sup></a>
                  </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">The Hebrew University of Jerusalem<sup>1</sup>, Tel Aviv University<sup>2</sup>, Google Research<sup>3</sup><br>ACL 2025</span>
                        <!--                        <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- Supplementary PDF link -->
                            <!--                            <span class="link-block">-->
                            <!--                      <a target="_blank"-->
                            <!--                         class="external-link button is-normal is-rounded is-dark">-->
                            <!--                      <span class="icon">-->
                            <!--                        <i class="fas fa-file-pdf"></i>-->
                            <!--                      </span>-->
                            <!--                      <span>Supplementary</span>-->
                            <!--                    </a>-->
                            <!--                  </span>-->

                            <!-- Github link -->
                            <span class="link-block">
  <a href="https://github.com/editinspector/EditInspector"
     target="_blank"
     rel="noopener noreferrer"
     class="external-link button is-normal is-rounded is-dark">
    <span class="icon">
      <i class="fab fa-github"></i>
    </span>
    <span>Code</span>
  </a>
</span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Edit Inspector Benchmark</h2>
        <!--        <h2 class="subtitle has-text-centered" style="padding: 10px; font-size: 1.3rem;">-->
        <!--            We introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human-->
        <!--            annotations collected using an extensive template for edit verification. EditInspector examines edits across-->
        <!--            five dimensions: (1) whether the edit accurately follows the instructions and aligns with user expectations;-->
        <!--            (2) presence of unintended artifacts; (3) technical quality, including resolution, sharpness, and clarity;-->
        <!--            (4) accuracy of the main difference description; (5) accuracy of a detailed listing of all differences-->
        <!--            between the original and the edited images.-->
        <!--        </h2>-->

        <div class="hero-body">
            <img src="static/images/model_answers.jpg">
            <h2 class="subtitle has-text-centered">
                Example of models evaluating the edit “Let the floor be made of wood.” Only few identified the edit
                executed correctly. Gemini 1.5 failed to detect any differences between the images. GPT-4o recognized
                the main floor change but missed additional edits, such as alterations to the fridge, door, and text on
                the yellow box.
            </h2>

            <!--            <h2 class="subtitle has-text-centered" style="padding: 10px; font-size: 1.3rem;">-->
            <!--                Using the EditInspector benchmark, we show that state-of-the-art vision-language models struggle as edit-->
            <!--                inspectors, with accuracy near random chance, and fail to effectively assess image edits. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation.-->
            <!--            </h2>-->
        </div>

    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Text-guided image editing, fueled by recent advancements in generative AI, is becoming
                        increasingly widespread. This trend highlights the need for a comprehensive framework to verify
                        text-guided edits and assess their quality. To address this need, we introduce EditInspector, a
                        novel benchmark for evaluation of text-guided image edits, based on human annotations collected
                        using an extensive template for edit verification. We leverage EditInspector to evaluate the
                        performance of state-of-the-art (SoTA) vision and language models in assessing edits across
                        various dimensions, including accuracy, artifact detection, visual quality, seamless integration
                        with the image scene, adherence to common sense, and the ability to describe edit-induced
                        changes. Our findings indicate that current models struggle to evaluate edits comprehensively
                        and frequently hallucinate when describing the changes. To address these challenges, we propose
                        two novel methods that outperform SoTA models in both artifact detection and difference caption
                        generation
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Teaser video-->
<section class="hero teaser" style="padding-top: 2rem;">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Human Evaluation Framework</h2>
        <div class="hero-body">
            <h2 class="subtitle has-text-centered" style="padding-top: 30px; text-align: left !important;">
            </h2>
            <img src="static/images/ui-example.jpg">

        </div>
    </div>
</section>

<section class="hero teaser is-light" style="padding-top: 2rem; ">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Evaluation Metrics</h2>
        <div class="hero-body" style="display: flex; justify-content: space-between; flex-direction: row-reverse;">
            <div class="figure-block" style="max-width: 45%;">
                <img src="static/images/new%20methods%20table.jpg">
                <h4 class=" has-text-centered">
                    Comparison of traditional metrics (BLEU, ROUGE, METEOR) against our proposed evaluation metric (MP).
                    The first example shows high scores despite missing the edited object. The second penalizes correct
                    but longer captions. The third fails to detect reversed edits, while our metric captures these
                    issues.
                </h4>
            </div>
            <div class="side-explanation"
                 style="max-width: 50%; font-size: 1.3em; font-weight: 500; line-height: 1.5; ">
                We propose two novel evaluation metrics tailored for differences caption comparisons:
                Model Precision (MP) and Hallucination Rate (HR). MP is the percentage of human-annotated differences
                matching model detected ones, while HR is the percentage of model detected differences that do not
                correspond to any human-annotated differences.
            </div>
        </div>
    </div>
</section>


<section class="hero teaser" style="padding-top: 2rem;">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Benchmark Table</h2>
        <div class="hero-body">
            <img src="static/images/benchmark%20.jpg">
            <h2 class="subtitle has-text-centered">
                Combined performance on Edit Inspectors questions, and the Difference Caption Generation task.
                Gemini-1.5 model demonstrates the best performance in Edit Inspectors questions. Qwen2.5-VL achieves the
                highest precision in predicting differences, with the lowest hallucination rate.
            </h2>

            Avg. Diff indicates the average number of differences detected per edit, while No Diffs represents the
            percentage of edits where no differences were predicted.
            <br/> Human annotators identified an average of 6 differences per edit. The main difference row reports the
            percentage of predicted main difference captions correctly describing the main difference.
        </div>
    </div>
</section>

<section class="hero teaser is-light" style="padding-top: 2rem;">
    <div class="container is-max-desktop">
        <h2 class="title is-3">New Methods: Difference Caption Generation</h2>
        <div class="hero-body">
            <img src="static/images/caption%20generation%20pipeline.jpg">
            <h2 class="subtitle has-text-centered">
                Example of our pipeline generating an instruction-grounded difference caption with rich metadata. Edit
                images are split into three zoom levels, with Gemini extracting and prioritizing captions to generate
                the metadata.
            </h2>
        </div>
    </div>
</section>

<section class="hero teaser" style="padding-top: 2rem;">
    <div class="container is-max-desktop">
        <h2 class="title is-3">New Methods: Artifact Detection</h2>
        <div class="hero-body">
            <img src="static/images/artifact-method.jpg">
            <div style="text-align: center">Edit: "turn the stop sign to a lollipop".</div>
            <h2 class="subtitle has-text-centered">
                We propose two novel methods for detecting artifacts using the Detic model. <br/> The first method
                compares segmentation probabilities for objects intersecting the turquoise in-painting mask between the
                pre-edit (left) and post-edit (right) images reveals two artifacts, the truck and small car, whose
                probability drops exceeds our threshold. The second method identifies elements that inter sect with the
                mask area, have disappeared from the image, and do not overlap with the edited object’s bounding box.
            </h2>
        </div>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p> Built using <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                       target="_blank">ACP Template</a>. </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
